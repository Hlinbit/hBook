{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing and Scheduling Vector Addition in TE for CPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm\n",
    "import tvm.testing\n",
    "from tvm import te\n",
    "import numpy as np\n",
    "import os\n",
    "from tvm.contrib import cc\n",
    "from tvm.contrib import utils\n",
    "\n",
    "tgt = tvm.target.Target(target=\"llvm\", host=\"llvm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Describing the Vector Computation\n",
    "\n",
    "TVM adopts tensor semantics, with each intermediate result represented as a multi-dimensional array. The user needs to describe the computation rule that generates the tensors.\n",
    "\n",
    "`n`: A symbolic variable to represent the shape. \n",
    "`A`: A placeholder Tensor, with given shape (n,)\n",
    "`B`: A placeholder Tensor, with given shape (n,)\n",
    "\n",
    "`C`: The result tensor, with a compute operation. \n",
    "\n",
    "`compute`: A computation, with the output conforming to the specified tensor shape and the computation to be performed at each position in the tensor defined by the lambda function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = te.var(\"n\")\n",
    "A = te.placeholder((n,), name=\"A\")\n",
    "B = te.placeholder((n,), name=\"B\")\n",
    "C = te.compute(A.shape, lambda i: A[i] + B[i], name=\"C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Default Schedule for the Computation\n",
    "\n",
    "While the above lines describe the computation rule, we can compute C in many different ways to fit different devices. For a tensor with multiple axes, you can choose which axis to iterate over first, or computations can be split across different threads. \n",
    "\n",
    "TVM requires that the user to provide a schedule, which is a description of how the computation should be performed. Scheduling operations within TE can change loop orders, split computations across different threads, and group blocks of data together, amongst other operations. An important concept behind schedules is that they only describe how the computation is performed, so different schedules for the same TE will produce the same result.\n",
    "\n",
    "## Naive\n",
    "\n",
    "TVM allows you to create a naive schedule that will compute C in by iterating in row major order.\n",
    "\n",
    "The `tvm.lower` command will generate the Intermediate Representation (IR) of the TE, with the corresponding schedule. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = te.create_schedule(C.op)\n",
    "print(tvm.lower(s, [A, B, C], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leverage Parallel \n",
    "\n",
    "A schedule is a series of steps that are applied to an expression to transform it in a number of different ways. When a schedule is applied to an expression in TE, the inputs and outputs remain the same, but when compiled the implementation of the expression can change. \n",
    "\n",
    "We can apply the parallel schedule operation to our computation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = te.create_schedule(C.op)\n",
    "s[C].parallel(C.op.axis[0])\n",
    "print(tvm.lower(s, [A, B, C], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leverage CPU Vectorization\n",
    "\n",
    "Modern CPUs also have the ability to perform SIMD operations on floating point values, and we can apply another schedule to our computation expression to take advantage of this. \n",
    "\n",
    "Accomplishing this requires multiple steps: \n",
    "- First we have to split the schedule into inner and outer loops using the split scheduling primitive. The inner loops can use vectorization to use SIMD instructions using the vectorize scheduling primitive, then the outer loops can be parallelized using the parallel scheduling primitive. \n",
    "- Choose the split factor to be the number of threads on your CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = te.create_schedule(C.op)\n",
    "\n",
    "# This factor should be chosen to match the number of threads appropriate for\n",
    "# your CPU. This will vary depending on architecture, but a good rule is\n",
    "# setting this factor to equal the number of available CPU cores.\n",
    "factor = 4\n",
    "\n",
    "outer, inner = s[C].split(C.op.axis[0], factor=factor)\n",
    "s[C].parallel(outer)\n",
    "s[C].vectorize(inner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complile the Schedule\n",
    "\n",
    "We use `tvm.build` to create a function. The build function takes the schedule, the desired signature of the function (including the inputs and outputs) as well as target language we want to compile to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Naive\n",
    "fadd = tvm.build(s, [A, B, C], tgt, name=\"fadd\")\n",
    "# For Parallelism\n",
    "fadd_parallel = tvm.build(s, [A, B, C], tgt, name=\"fadd_parallel\")\n",
    "# For Vectorization\n",
    "fadd_vector = tvm.build(s, [A, B, C], tgt, name=\"fadd_vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the Schedule\n",
    "\n",
    "Run Schedule："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = tvm.device(tgt.kind.name, 0)\n",
    "\n",
    "n = 1024\n",
    "a = tvm.nd.array(np.random.uniform(size=n).astype(A.dtype), dev)\n",
    "b = tvm.nd.array(np.random.uniform(size=n).astype(B.dtype), dev)\n",
    "c = tvm.nd.array(np.zeros(n, dtype=C.dtype), dev)\n",
    "fadd(a, b, c)\n",
    "tvm.testing.assert_allclose(c.numpy(), a.numpy() + b.numpy())\n",
    "fadd_parallel(a, b, c)\n",
    "tvm.testing.assert_allclose(c.numpy(), a.numpy() + b.numpy())\n",
    "fadd_vector(a, b ,c)\n",
    "tvm.testing.assert_allclose(c.numpy(), a.numpy() + b.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a evaluation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "np_repeat = 100\n",
    "np_running_time = timeit.timeit(\n",
    "    setup=\"import numpy\\n\"\n",
    "    \"n = 32768\\n\"\n",
    "    'dtype = \"float32\"\\n'\n",
    "    \"a = numpy.random.rand(n, 1).astype(dtype)\\n\"\n",
    "    \"b = numpy.random.rand(n, 1).astype(dtype)\\n\",\n",
    "    stmt=\"answer = a + b\",\n",
    "    number=np_repeat,\n",
    ")\n",
    "print(\"Numpy running time: %f\" % (np_running_time / np_repeat))\n",
    "\n",
    "\n",
    "def evaluate_addition(func, target, optimization, log):\n",
    "    dev = tvm.device(target.kind.name, 0)\n",
    "    n = 32768\n",
    "    a = tvm.nd.array(np.random.uniform(size=n).astype(A.dtype), dev)\n",
    "    b = tvm.nd.array(np.random.uniform(size=n).astype(B.dtype), dev)\n",
    "    c = tvm.nd.array(np.zeros(n, dtype=C.dtype), dev)\n",
    "\n",
    "    evaluator = func.time_evaluator(func.entry_name, dev, number=10)\n",
    "    mean_time = evaluator(a, b, c).mean\n",
    "    print(\"%s: %f\" % (optimization, mean_time))\n",
    "\n",
    "    log.append((optimization, mean_time))\n",
    "\n",
    "\n",
    "log = [(\"numpy\", np_running_time / np_repeat)]\n",
    "evaluate_addition(fadd, tgt, \"naive\", log=log)\n",
    "evaluate_addition(fadd_parallel, tgt, \"parallel\", log=log)\n",
    "evaluate_addition(fadd_vector, tgt, \"vector\", log=log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Loading Compiled Modules\n",
    "\n",
    "## Saving the compiled modules\n",
    "\n",
    "The following code first performs the following steps:\n",
    "\n",
    "- It saves the compiled host module into an object file.\n",
    "\n",
    "- Then it saves the device module into a ptx file.\n",
    "\n",
    "- `cc.create_shared` calls a compiler (gcc) to create a shared library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_path = \"tmp\"\n",
    "os.makedirs(tmp_path, exist_ok=True)\n",
    "fadd.save(os.path.join(tmp_path, \"myadd.o\"))\n",
    "\n",
    "if tgt.kind.name == \"cuda\":\n",
    "    fadd.imported_modules[0].save(os.path.join(tmp_path,\"myadd.ptx\"))\n",
    "if tgt.kind.name == \"rocm\":\n",
    "    fadd.imported_modules[0].save(os.path.join(tmp_path,\"myadd.hsaco\"))\n",
    "if tgt.kind.name.startswith(\"opencl\"):\n",
    "    fadd.imported_modules[0].save(os.path.join(tmp_path,\"myadd.cl\"))\n",
    "cc.create_shared(os.path.join(tmp_path,\"myadd.so\"), [os.path.join(tmp_path,\"myadd.o\")])\n",
    "print(os.listdir(tmp_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Compiled Module¶\n",
    "\n",
    "We can load the compiled module from the file system and run the code. The following code loads the host and device module separately and links them together. We can verify that the newly loaded function works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fadd1 = tvm.runtime.load_module(os.path.join(tmp_path, \"myadd.so\"))\n",
    "if tgt.kind.name == \"cuda\":\n",
    "    fadd1_dev = tvm.runtime.load_module(os.path.join(tmp_path, \"myadd.ptx\"))\n",
    "    fadd1.import_module(fadd1_dev)\n",
    "\n",
    "if tgt.kind.name == \"rocm\":\n",
    "    fadd1_dev = tvm.runtime.load_module(os.path.join(tmp_path, \"myadd.hsaco\"))\n",
    "    fadd1.import_module(fadd1_dev)\n",
    "\n",
    "if tgt.kind.name.startswith(\"opencl\"):\n",
    "    fadd1_dev = tvm.runtime.load_module(os.path.join(tmp_path, \"myadd.cl\"))\n",
    "    fadd1.import_module(fadd1_dev)\n",
    "\n",
    "fadd1(a, b, c)\n",
    "tvm.testing.assert_allclose(c.numpy(), a.numpy() + b.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
